{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-18 12:56:41.068195: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/ant/miniconda3/envs/psl/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<pad>': 1,\n",
       " '<start>': 2,\n",
       " '<eos>': 3,\n",
       " 'j': 4,\n",
       " 'r': 5,\n",
       " 'z': 6,\n",
       " 't': 7,\n",
       " 's': 8,\n",
       " 'n': 9,\n",
       " 'g': 10,\n",
       " 'b': 11,\n",
       " 'l': 12,\n",
       " 'y': 13,\n",
       " 'ch': 14,\n",
       " 'u': 15,\n",
       " 'ó': 16,\n",
       " 'd': 17,\n",
       " 'f': 18,\n",
       " 'ż': 19,\n",
       " 'k': 20,\n",
       " 'e': 21,\n",
       " 'cz': 22,\n",
       " 'sz': 23,\n",
       " 'o': 24,\n",
       " 'ź': 25,\n",
       " 'm': 26,\n",
       " 'ń': 27,\n",
       " 'ć': 28,\n",
       " 'c': 29,\n",
       " 'ę': 30,\n",
       " 'i': 31,\n",
       " 'ł': 32,\n",
       " 'ą': 33,\n",
       " 'w': 34,\n",
       " 'h': 35,\n",
       " 'ś': 36,\n",
       " 'rz': 37,\n",
       " 'a': 38,\n",
       " 'p': 39}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a list of lowercase letters from the filenames in the specified directory\n",
    "letters = [l.split('.')[0].lower() for l in os.listdir(\"/home/ant/projects/psl/dataset/Videos/alphabet\")]\n",
    "\n",
    "# Define the vocabulary as a list containing '<start>' and '<eos>' tokens, along with the letters\n",
    "vocabulary = ['<pad>', '<start>', '<eos>'] + letters\n",
    "\n",
    "# Create a dictionary mapping each vocabulary item to its corresponding index\n",
    "# Indexing starts from 1, so '<start>' is assigned index 1, '<eos>' is assigned index 2, and so on\n",
    "vocabulary = {l: i+1 for i, l in enumerate(vocabulary)}\n",
    "\n",
    "# Display the resulting vocabulary dictionary\n",
    "vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract hand landmarks from a video\n",
    "def landmarks_timeseries(video_path):\n",
    "    mp_hands = mp.solutions.hands\n",
    "    hands = mp_hands.Hands()\n",
    "\n",
    "    # Open the video file for reading\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    # Get the total number of frames in the video\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    # Get the frames per second (fps) of the video\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "    # Set the frame rate for extracting landmarks\n",
    "    frame_rate = 0.5\n",
    "\n",
    "    # Calculate the number of frames to skip based on the frame rate\n",
    "    frames_to_skip = int(fps * frame_rate)\n",
    "\n",
    "    landmarks_data = []\n",
    "    current_frame = 0\n",
    "\n",
    "    # Loop through the frames of the video\n",
    "    while cap.isOpened():\n",
    "        # Set the position to the current frame\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, current_frame)\n",
    "\n",
    "        # Read the current frame from the video\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Process the frame and get hand landmarks using Mediapipe\n",
    "        results = hands.process(rgb_frame)\n",
    "        if results.multi_hand_landmarks:\n",
    "            hand_landmarks = results.multi_hand_landmarks[0].landmark\n",
    "\n",
    "            # Append the 3D coordinates of hand landmarks to the list\n",
    "            landmarks_data.append([[landmark.x, landmark.y, landmark.z] for landmark in hand_landmarks])\n",
    "\n",
    "        # Move to the next frame based on the frames to skip\n",
    "        current_frame += frames_to_skip\n",
    "\n",
    "    # Release the video capture object\n",
    "    cap.release()\n",
    "\n",
    "    # Reshape the landmarks data into a 2D array\n",
    "    landmarks_data = np.array(landmarks_data).reshape(len(landmarks_data), -1)\n",
    "\n",
    "    return landmarks_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/36 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1705582603.316787   19478 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1705582603.334920   19515 gl_context.cc:344] GL version: 3.2 (OpenGL ES 3.2 NVIDIA 535.146.02), renderer: NVIDIA GeForce GTX 970/PCIe/SSE2\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "  3%|▎         | 1/36 [00:00<00:25,  1.38it/s]I0000 00:00:1705582604.031034   19478 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1705582604.037516   19539 gl_context.cc:344] GL version: 3.2 (OpenGL ES 3.2 NVIDIA 535.146.02), renderer: NVIDIA GeForce GTX 970/PCIe/SSE2\n",
      "  6%|▌         | 2/36 [00:01<00:25,  1.33it/s]I0000 00:00:1705582604.794730   19478 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1705582604.801071   19556 gl_context.cc:344] GL version: 3.2 (OpenGL ES 3.2 NVIDIA 535.146.02), renderer: NVIDIA GeForce GTX 970/PCIe/SSE2\n",
      "  6%|▌         | 2/36 [00:02<00:36,  1.09s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m labels\u001b[38;5;241m.\u001b[39mappend(label)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Call the function 'landmarks_timeseries' to get landmarks from the video\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m land \u001b[38;5;241m=\u001b[39m \u001b[43mlandmarks_timeseries\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m landmarks\u001b[38;5;241m.\u001b[39mappend(land)\n",
      "Cell \u001b[0;32mIn[3], line 37\u001b[0m, in \u001b[0;36mlandmarks_timeseries\u001b[0;34m(video_path)\u001b[0m\n\u001b[1;32m     34\u001b[0m rgb_frame \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(frame, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2RGB)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Process the frame and get hand landmarks using Mediapipe\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mhands\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrgb_frame\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m results\u001b[38;5;241m.\u001b[39mmulti_hand_landmarks:\n\u001b[1;32m     39\u001b[0m     hand_landmarks \u001b[38;5;241m=\u001b[39m results\u001b[38;5;241m.\u001b[39mmulti_hand_landmarks[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mlandmark\n",
      "File \u001b[0;32m~/miniconda3/envs/psl/lib/python3.10/site-packages/mediapipe/python/solutions/hands.py:153\u001b[0m, in \u001b[0;36mHands.process\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess\u001b[39m(\u001b[38;5;28mself\u001b[39m, image: np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NamedTuple:\n\u001b[1;32m    133\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Processes an RGB image and returns the hand landmarks and handedness of each detected hand.\u001b[39;00m\n\u001b[1;32m    134\u001b[0m \n\u001b[1;32m    135\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;124;03m         right hand) of the detected hand.\u001b[39;00m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimage\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/psl/lib/python3.10/site-packages/mediapipe/python/solution_base.py:372\u001b[0m, in \u001b[0;36mSolutionBase.process\u001b[0;34m(self, input_data)\u001b[0m\n\u001b[1;32m    366\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    367\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph\u001b[38;5;241m.\u001b[39madd_packet_to_input_stream(\n\u001b[1;32m    368\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream_name,\n\u001b[1;32m    369\u001b[0m         packet\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_packet(input_stream_type,\n\u001b[1;32m    370\u001b[0m                                  data)\u001b[38;5;241m.\u001b[39mat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_simulated_timestamp))\n\u001b[0;32m--> 372\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_graph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait_until_idle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;66;03m# Create a NamedTuple object where the field names are mapping to the graph\u001b[39;00m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;66;03m# output stream names.\u001b[39;00m\n\u001b[1;32m    375\u001b[0m solution_outputs \u001b[38;5;241m=\u001b[39m collections\u001b[38;5;241m.\u001b[39mnamedtuple(\n\u001b[1;32m    376\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSolutionOutputs\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_stream_type_info\u001b[38;5;241m.\u001b[39mkeys())\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "videos_path = \"/home/ant/projects/psl/dataset/Videos/alphabet\"\n",
    "labels = []\n",
    "landmarks = []\n",
    "# Iterate through each file in the dynamic alphabet directory\n",
    "for i, filename in enumerate(tqdm(os.listdir(videos_path))):\n",
    "    if filename.endswith('.mp4'):\n",
    "        video_path = os.path.join(videos_path, filename)\n",
    "\n",
    "        label = filename.split('.')[0].lower()\n",
    "        label = ['<start>', label, '<eos>']\n",
    "\n",
    "        # Convert labels to their corresponding vocabulary indices\n",
    "        label = [vocabulary[l] for l in label]\n",
    "        labels.append(label)\n",
    "\n",
    "        # Call the function 'landmarks_timeseries' to get landmarks from the video\n",
    "        land = landmarks_timeseries(video_path)\n",
    "\n",
    "        landmarks.append(land)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/375 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1705582609.469689   19478 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1705582609.479287   19574 gl_context.cc:344] GL version: 3.2 (OpenGL ES 3.2 NVIDIA 535.146.02), renderer: NVIDIA GeForce GTX 970/PCIe/SSE2\n",
      "  0%|          | 1/375 [00:01<06:30,  1.04s/it]I0000 00:00:1705582610.510816   19478 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1705582610.517787   19591 gl_context.cc:344] GL version: 3.2 (OpenGL ES 3.2 NVIDIA 535.146.02), renderer: NVIDIA GeForce GTX 970/PCIe/SSE2\n",
      "  0%|          | 1/375 [00:01<11:11,  1.80s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m labels_words\u001b[38;5;241m.\u001b[39mappend(label)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Call the function 'landmarks_timeseries' to get landmarks from the video\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m land \u001b[38;5;241m=\u001b[39m \u001b[43mlandmarks_timeseries\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m landmarks_words\u001b[38;5;241m.\u001b[39mappend(land)\n",
      "Cell \u001b[0;32mIn[3], line 27\u001b[0m, in \u001b[0;36mlandmarks_timeseries\u001b[0;34m(video_path)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Loop through the frames of the video\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m cap\u001b[38;5;241m.\u001b[39misOpened():\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;66;03m# Set the position to the current frame\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m     \u001b[43mcap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCAP_PROP_POS_FRAMES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_frame\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;66;03m# Read the current frame from the video\u001b[39;00m\n\u001b[1;32m     30\u001b[0m     ret, frame \u001b[38;5;241m=\u001b[39m cap\u001b[38;5;241m.\u001b[39mread()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "videos_path = \"/home/ant/projects/psl/dataset/Videos/words\"\n",
    "labels_words = []\n",
    "landmarks_words = []\n",
    "# Iterate through each file in the words directory\n",
    "for i, filename in enumerate(tqdm(os.listdir(videos_path))):\n",
    "    if filename.endswith('.mp4'):\n",
    "        video_path = os.path.join(videos_path, filename)\n",
    "\n",
    "        # Extract labels from the filename, including '<start>' and '<eos>' tokens\n",
    "        label = ['<start>'] + list(filename.split('.')[0].lower()) + ['<eos>']\n",
    "\n",
    "        # Convert labels to their corresponding vocabulary indices\n",
    "        label = [vocabulary[l] for l in label]\n",
    "\n",
    "        labels_words.append(label)\n",
    "\n",
    "        # Call the function 'landmarks_timeseries' to get landmarks from the video\n",
    "        land = landmarks_timeseries(video_path)\n",
    "        landmarks_words.append(land)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3626/3626 [00:00<00:00, 18467.71it/s]\n"
     ]
    }
   ],
   "source": [
    "# Define a mapping to fix certain characters in the labels\n",
    "fix = {\n",
    "    'Ć': 'ć',\n",
    "    'Ę': 'ę',\n",
    "    'Ł': 'ł',\n",
    "    'Ń': 'ń',\n",
    "    'Ó': 'O',\n",
    "    'Ś': 'ś',\n",
    "    'Ź': 'ź',\n",
    "    'Ż': 'ż',\n",
    "}\n",
    "\n",
    "# Function to preprocess data from JSON files in the folder\n",
    "def preprocess_data(labels_folder):\n",
    "    labeled_with_landmarks_count = 0\n",
    "    labeled_without_landmarks_count = 0\n",
    "    data_rows = []  # List to store data rows\n",
    "    labels = []  # List to store labels\n",
    "\n",
    "    # Loop through JSON files in the folder\n",
    "    for filename in tqdm(os.listdir(labels_folder)):\n",
    "        if filename.endswith('.json'):\n",
    "            with open(os.path.join(labels_folder, filename), 'r', encoding='utf-8') as json_file:\n",
    "                data = json.load(json_file)\n",
    "                if 'hand_landmarks' in data:\n",
    "                    labeled_with_landmarks_count += 1\n",
    "\n",
    "                    # Extract landmarks data and flatten it into a list\n",
    "                    landmarks_data = data['hand_landmarks']\n",
    "                    row = []\n",
    "                    for landmark_key in landmarks_data:\n",
    "                        landmark = landmarks_data[landmark_key]\n",
    "                        row.extend([landmark['x'], landmark['y'], landmark['z']])\n",
    "\n",
    "                    # Extract and preprocess the label\n",
    "                    l = data['label']\n",
    "                    if l in fix:\n",
    "                        l = fix[l]\n",
    "                    label = ['<start>', l.lower(), '<eos>']\n",
    "                    label = [vocabulary[l] for l in label]\n",
    "                    data_rows.append(row)\n",
    "                    labels.append(label)\n",
    "\n",
    "                else:\n",
    "                    labeled_without_landmarks_count += 1\n",
    "\n",
    "    return data_rows, labels\n",
    "\n",
    "labels_folder = \"/home/ant/projects/psl/dataset/labels\"\n",
    "preprocessed_data, preprocessed_labels = preprocess_data(labels_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3626/3626 [00:00<00:00, 21445.95it/s]\n"
     ]
    }
   ],
   "source": [
    "labels_folder = '../dataset/labels'\n",
    "data_static, labels_static = preprocess_data(labels_folder)\n",
    "data_static = np.array(data_static)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new list to store the modified static data\n",
    "data_static_new = []\n",
    "\n",
    "# Iterate through each element in the original static data\n",
    "for d in data_static:\n",
    "    # Repeat the current element along a new axis a random number of times (between 2 and 6)\n",
    "    d = np.repeat(d.reshape(1, -1), repeats=np.random.randint(2, 7), axis=0)\n",
    "    data_static_new.append(d)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all lists\n",
    "data = landmarks + landmarks_words + data_static_new \n",
    "all_labels = labels + labels_words + labels_static"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_static_new \n",
    "all_labels = labels_static"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_loss(y_true, y_pred):\n",
    "    # Initialize SparseCategoricalCrossentropy loss with 'from_logits' and 'reduction' parameters\n",
    "    loss_function = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "        from_logits=True, reduction='none')\n",
    "    \n",
    "    # Calculate the cross-entropy loss for each item in the batch\n",
    "    loss = loss_function(y_true - 1, y_pred)\n",
    "\n",
    "    # Create a binary mask to filter out padding elements (where y_true is 0)\n",
    "    mask = tf.cast(y_true != 0, tf.float32)\n",
    "\n",
    "    # Apply the mask to the calculated losses\n",
    "    loss *= mask\n",
    "\n",
    "    return tf.reduce_sum(loss) / tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_for_letters(y_true, y_pred):\n",
    "    result = tf.cast(tf.cast(y_true, tf.int64) == tf.argmax(y_pred, axis=-1), tf.float32)\n",
    "\n",
    "    # Create a binary mask to filter out padding elements (where y_true is 0)\n",
    "    mask = tf.cast(y_true != 0, tf.float32)\n",
    "\n",
    "    # Apply the mask to the calculated losses\n",
    "    result *= mask\n",
    "\n",
    "    return tf.reduce_sum(result) / tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3262, 6, 63), (3262, 3))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pad input sequences (data) with zeros using \"post\" padding\n",
    "padded_inputs = tf.keras.utils.pad_sequences(data, dtype=\"float32\", padding=\"post\")\n",
    "padded_outputs = tf.keras.utils.pad_sequences(all_labels, dtype=\"int32\", padding=\"post\")\n",
    "padded_inputs.shape, padded_outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " masking_5 (Masking)         (None, 6, 63)             0         \n",
      "                                                                 \n",
      " lstm_10 (LSTM)              (None, 32)                12288     \n",
      "                                                                 \n",
      " repeat_vector_5 (RepeatVect  (None, 3, 32)            0         \n",
      " or)                                                             \n",
      "                                                                 \n",
      " lstm_11 (LSTM)              (None, 3, 64)             24832     \n",
      "                                                                 \n",
      " time_distributed_5 (TimeDis  (None, 3, 39)            2535      \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 39,655\n",
      "Trainable params: 39,655\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "\n",
    "# Define the input dimension, vocabulary size, and create a Sequential model\n",
    "input_dim = 63\n",
    "vocab_size = len(vocabulary)\n",
    "\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        # Input layer with shape (sequence_length, input_dim)\n",
    "        keras.Input(shape=(padded_inputs.shape[1], input_dim), dtype=\"float32\"),\n",
    "\n",
    "        # Masking layer to handle variable-length sequences\n",
    "        keras.layers.Masking(),\n",
    "\n",
    "        # LSTM layer with 32 units, returning a single output for each sequence\n",
    "        keras.layers.LSTM(32, return_sequences=False),\n",
    "\n",
    "        # Repeat the output vector for each time step in the output sequence\n",
    "        keras.layers.RepeatVector(padded_outputs.shape[1]),\n",
    "\n",
    "        # LSTM layer with 64 units, returning a sequence of vectors\n",
    "        keras.layers.LSTM(64, return_sequences=True),\n",
    "\n",
    "        # TimeDistributed layer to apply Dense layer to each time step independently\n",
    "        keras.layers.TimeDistributed(keras.layers.Dense(vocab_size)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Display the model summary\n",
    "model.summary()\n",
    "\n",
    "# Compile the model using the custom masked loss function and Adam optimizer\n",
    "model.compile(\n",
    "    loss=masked_loss,\n",
    "    optimizer=tf.keras.optimizers.legacy.Adam(1e-3),\n",
    "    metrics=[accuracy_for_letters]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102/102 [==============================] - 3s 4ms/step - loss: 2.2096 - accuracy_for_letters: 0.1343\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3e941bf940>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(padded_inputs, padded_outputs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "psl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
